# 1. Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹
# ----------------------
import subprocess
import sys


# ----------------------
# 2. Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñ‹ Ğ¸ ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ñ‹
# ----------------------
import pandas as pd
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import json
from openai import AsyncOpenAI
import asyncio
import numpy as np
import os
from supabase import create_client, Client

# ĞšĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ API DeepSeek
DEESEEK_API_URL = "https://api.studio.nebius.ai/v1/"

# ----------------------
# 3. Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
# ----------------------
@st.cache_data
def load_data(file_path: str) -> pd.DataFrame:
    df = pd.read_excel(file_path)
    df.columns = [str(col).strip().lower() if isinstance(col, str) else col for col in df.columns]
    if 'data' in df.columns:
        df['data'] = pd.to_datetime(df['data'], errors='coerce')
    return df

# ----------------------
# 4. DeepSeek API Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·
# ----------------------
async def analyze_reflection_with_deepseek(client: AsyncOpenAI, text: str) -> dict:
    error_result = {
        "sentiment_score": 0.0, "learning_feedback": "N/A", "teamwork_feedback": "N/A",
        "organization_feedback": "N/A", "learning_sentiment_score": 0.0,
        "teamwork_sentiment_score": 0.0, "organization_sentiment_score": 0.0,
    }
    if not text or not isinstance(text, str) or not text.strip():
        return error_result
    prompt = (
        "Ğ¢Ñ‹ â€” Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸. ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ ÑˆĞºĞ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ°. "
        "Ğ¢Ğ²Ğ¾Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° â€” Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ JSON-Ğ¾Ğ±ÑŠĞµĞºÑ‚ ÑĞ¾ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ĞºĞ»ÑÑ‡Ğ°Ğ¼Ğ¸:\n"
        "1. 'sentiment_score': Ğ¾Ğ±Ñ‰Ğ°Ñ Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ¾Ñ‚ -1.0 Ğ´Ğ¾ 1.0.\n"
        "2. 'learning_feedback': ĞºÑ€Ğ°Ñ‚ĞºĞ°Ñ Ğ²Ñ‹Ğ¶Ğ¸Ğ¼ĞºĞ° (1-2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ) Ğ¾Ğ± Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°.\n"
        "3. 'teamwork_feedback': ĞºÑ€Ğ°Ñ‚ĞºĞ°Ñ Ğ²Ñ‹Ğ¶Ğ¸Ğ¼ĞºĞ° (1-2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ) Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ² ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğµ.\n"
        "4. 'organization_feedback': ĞºÑ€Ğ°Ñ‚ĞºĞ°Ñ Ğ²Ñ‹Ğ¶Ğ¸Ğ¼ĞºĞ° (1-2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ) Ğ¾Ğ± Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑƒĞ³Ğµ.\n"
        "5. 'learning_sentiment_score': Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¢ĞĞ›Ğ¬ĞšĞ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ ÑƒÑ‡Ñ‘Ğ±Ñƒ (Ğ¾Ñ‚ -1.0 Ğ´Ğ¾ 1.0). Ğ•ÑĞ»Ğ¸ Ğ½Ğµ ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ĞµÑ‚ÑÑ, Ğ²ĞµÑ€Ğ½Ğ¸ 0.0.\n"
        "6. 'teamwork_sentiment_score': Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¢ĞĞ›Ğ¬ĞšĞ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ (Ğ¾Ñ‚ -1.0 Ğ´Ğ¾ 1.0). Ğ•ÑĞ»Ğ¸ Ğ½Ğµ ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ĞµÑ‚ÑÑ, Ğ²ĞµÑ€Ğ½Ğ¸ 0.0.\n"
        "7. 'organization_sentiment_score': Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¢ĞĞ›Ğ¬ĞšĞ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (Ğ¾Ñ‚ -1.0 Ğ´Ğ¾ 1.0). Ğ•ÑĞ»Ğ¸ Ğ½Ğµ ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ĞµÑ‚ÑÑ, Ğ²ĞµÑ€Ğ½Ğ¸ 0.0.\n\n"
        f"Ğ¢ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°: \"{text}\""
    )
    try:
        response = await client.chat.completions.create(
            model="deepseek-ai/DeepSeek-V3", messages=[{"role": "user", "content": prompt}],
            temperature=0.2, response_format={"type": "json_object"}
        )
        result = json.loads(response.choices[0].message.content)
        for key, value in error_result.items():
            if key not in result: result[key] = value
        return result
    except Exception as e:
        print(f"Error processing text: '{text[:50]}...'. Error: {e}")
        return error_result

# ----------------------
# 5. ĞĞ¾Ğ²Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸
# ----------------------
async def _get_one_nomination(client: AsyncOpenAI, username: str, text: str) -> dict:
    prompt = (
        "Ğ¢Ñ‹ â€” Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ ÑˆĞºĞ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ¾Ğ² Ñ Ğ¼Ğ¾Ñ€ÑĞºĞ¾Ğ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾-Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¼ĞµĞ½Ñ‹. "
        f"ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¹ ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ° Ğ¿Ğ¾ Ğ¸Ğ¼ĞµĞ½Ğ¸ {username}: \"{text}\", Ğ¿Ñ€Ğ¸ÑĞ²Ğ¾Ğ¹ ÑˆÑƒÑ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ½Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¼Ğ¾Ñ€ÑĞºĞ¾Ğ¹ Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ "
        "(Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, 'ĞšĞ°Ğ¿Ğ¸Ñ‚Ğ°Ğ½ Ğ“ĞµĞ½Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'Ğ˜Ğ½Ğ¶ĞµĞ½ĞµÑ€ Ğ“Ğ»ÑƒĞ±Ğ¸Ğ½') Ğ¸ Ğ´Ğ°Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (1-2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ). "
        "ĞĞ¾Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑˆĞºĞ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ¾Ğ². "
        "Ğ’ĞµÑ€Ğ½Ğ¸ JSON-Ğ¾Ğ±ÑŠĞµĞºÑ‚: {\"nomination\": str, \"justification\": str}."
    )
    default_result = {"nomination": "ĞœĞ¾Ñ€ÑĞºĞ¾Ğ¹ Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ", "justification": "Ğ—Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğµ!"}
    try:
        response = await client.chat.completions.create(model="deepseek-ai/DeepSeek-V3", messages=[{"role": "user", "content": prompt}], temperature=0.7, response_format={"type": "json_object"})
        result = json.loads(response.choices[0].message.content)
        return result if 'nomination' in result and 'justification' in result else default_result
    except Exception as e:
        print(f"Error generating nomination for {username}: {e}")
        return default_result

async def _generate_nominations_async(_df: pd.DataFrame, client: AsyncOpenAI) -> pd.DataFrame:
    user_reflections = _df.groupby('username')['text'].apply(lambda texts: ' '.join(texts.astype(str).str.strip())).reset_index()
    tasks = [_get_one_nomination(client, row['username'], row['text']) for _, row in user_reflections.iterrows()]
    results = await asyncio.gather(*tasks)
    results_df = pd.DataFrame(results)
    return pd.concat([user_reflections[['username']], results_df], axis=1).rename(columns={'username': 'Ğ¤Ğ˜Ğ', 'nomination': 'ĞĞ¾Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ', 'justification': 'ĞĞ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ'})

@st.cache_data(show_spinner=False, hash_funcs={AsyncOpenAI: lambda _: None})
def get_cached_nominations(_df: pd.DataFrame, client: AsyncOpenAI) -> pd.DataFrame:
    return asyncio.run(_generate_nominations_async(_df, client))

async def _get_one_friendly_reflection(client: AsyncOpenAI, username: str, text: str) -> dict:
    prompt = (
        "Ğ¢Ñ‹ â€” Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚, ÑÑƒĞ¼Ğ¼Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ ÑˆĞºĞ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ¾Ğ² Ñ Ğ¼Ğ¾Ñ€ÑĞºĞ¾Ğ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾-Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¼ĞµĞ½Ñ‹. "
        f"ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¹ ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ° Ğ¿Ğ¾ Ğ¸Ğ¼ĞµĞ½Ğ¸ {username}: \"{text}\", ÑĞ¾Ğ·Ğ´Ğ°Ğ¹ Ğ´Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ğ¾Ğµ, ÑˆÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ (2-3 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ) "
        "Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ½Ğ°Ğ¿ÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ (1-2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ). Ğ¢Ğ¾Ğ½ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼, Ğ½Ğµ Ğ¾Ğ±Ğ¸Ğ´Ğ½Ñ‹Ğ¼, Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ñ€ÑĞºĞ¾Ğ¹ Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸. "
        "Ğ’ĞµÑ€Ğ½Ğ¸ JSON-Ğ¾Ğ±ÑŠĞµĞºÑ‚: {\"reflection\": str, \"encouragement\": str}."
    )
    default_result = {"reflection": "Ğ¢Ñ‹ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑˆÑŒÑÑ Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ğ¼Ğ¸!", "encouragement": "ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°Ğ¹ Ğ² Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ´ÑƒÑ…Ğµ Ğ¸ Ğ¿Ğ¾ĞºĞ¾Ñ€ÑĞ¹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹!"}
    try:
        response = await client.chat.completions.create(model="deepseek-ai/DeepSeek-V3", messages=[{"role": "user", "content": prompt}], temperature=0.7, response_format={"type": "json_object"})
        result = json.loads(response.choices[0].message.content)
        return result if 'reflection' in result and 'encouragement' in result else default_result
    except Exception as e:
        print(f"Error generating friendly reflection for {username}: {e}")
        return default_result

async def _generate_friendly_reflections_async(_df: pd.DataFrame, client: AsyncOpenAI) -> pd.DataFrame:
    user_reflections = _df.groupby('username')['text'].apply(lambda texts: ' '.join(texts.astype(str).str.strip())).reset_index()
    tasks = [_get_one_friendly_reflection(client, row['username'], row['text']) for _, row in user_reflections.iterrows()]
    results = await asyncio.gather(*tasks)
    results_df = pd.DataFrame(results)
    return pd.concat([user_reflections[['username']], results_df], axis=1).rename(columns={'username': 'Ğ¤Ğ˜Ğ', 'reflection': 'Ğ ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ', 'encouragement': 'ĞŸĞ¾Ğ¶ĞµĞ»Ğ°Ğ½Ğ¸Ğµ'})

@st.cache_data(show_spinner=False, hash_funcs={AsyncOpenAI: lambda _: None})
def get_cached_friendly_reflections(_df: pd.DataFrame, client: AsyncOpenAI) -> pd.DataFrame:
    return asyncio.run(_generate_friendly_reflections_async(_df, client))

# ----------------------
# 6. Ğ’ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸
# ----------------------
def convert_sentiment_to_10_point(score: float) -> float:
    return (score + 1) * 4.5 + 1 if isinstance(score, (int, float)) else 5.5

@st.cache_resource
def init_supabase_client():
    try:
        return create_client(st.secrets["SUPABASE_URL"], st.secrets["SUPABASE_KEY"])
    except Exception as e:
        st.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğº Supabase: {e}")
        return None

@st.cache_data(ttl=300)
def get_report_list_from_supabase(_supabase: Client) -> list:
    try:
        response = _supabase.table('reports').select('report_name', count='exact').execute()
        return sorted(list(set(item['report_name'] for item in response.data)), reverse=True) if response.data else []
    except Exception as e:
        st.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ¿Ğ¸ÑĞºĞ° Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ² Ğ¸Ğ· Supabase: {e}")
        return []

@st.cache_data
def load_report_from_supabase(_supabase: Client, report_name: str) -> pd.DataFrame:
    try:
        response = _supabase.table('reports').select('*').eq('report_name', report_name).execute()
        df = pd.DataFrame(response.data)
        if 'data' in df.columns:
            df['data'] = pd.to_datetime(df['data'], errors='coerce')
        return df.drop(columns=['id', 'created_at', 'report_name'], errors='ignore') if not df.empty else df
    except Exception as e:
        st.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ° '{report_name}': {e}")
        return pd.DataFrame()

# ----------------------
# 7. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ğ¸ Ğ´Ğ°ÑˆĞ±Ğ¾Ñ€Ğ´
# ----------------------
def main():
    st.set_page_config(layout="wide")
    st.title("Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´Ğ°ÑˆĞ±Ğ¾Ñ€Ğ´ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¹ ÑƒÑ‡Ğ°Ñ‰Ğ¸Ñ…ÑÑ")

    with st.expander("â„¹ï¸ Ğ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğµ: Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ğ¸ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ?", expanded=False):
        st.markdown("""...""") # Ğ¡ĞºÑ€Ñ‹Ğ» Ğ´Ğ»Ñ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ‚Ğ¸

    supabase = init_supabase_client()
    if not supabase: st.stop()

    st.sidebar.header("ğŸ—‚ï¸ Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
    report_files = get_report_list_from_supabase(supabase)
    data_source_options = ["ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·"] + report_files
    selected_source = st.sidebar.selectbox("Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚:", data_source_options)

    df = None
    uploaded_file = None
    
    if selected_source == "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·":
        uploaded_file = st.sidebar.file_uploader("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Excel-Ñ„Ğ°Ğ¹Ğ»:", type="xlsx")
        if uploaded_file:
            df = load_data(uploaded_file)
            st.session_state['current_file_name'] = uploaded_file.name
    else:
        df = load_report_from_supabase(supabase, selected_source)
        st.session_state['current_file_name'] = selected_source

    # Ğ¡Ğ±Ñ€Ğ¾Ñ Ñ„Ğ»Ğ°Ğ³Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¼ĞµĞ½Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    file_key = st.session_state.get('current_file_name')
    if 'last_file_key' not in st.session_state or st.session_state.last_file_key != file_key:
        st.session_state.show_nominations = False
        st.session_state.show_reflections = False
        st.session_state.last_file_key = file_key

    if df is None:
        st.info("ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ñ„Ğ°Ğ¹Ğ» Ğ¸Ğ»Ğ¸ Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚.")
        return

    client = AsyncOpenAI(base_url=DEESEEK_API_URL, api_key=st.secrets["DEEPSEEK_API_KEY"]) if selected_source == "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·" else None

    session_key = f"df_processed_{st.session_state.get('current_file_name', 'default')}"
    if session_key not in st.session_state:
        if selected_source == "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·" and client:
            with st.spinner('Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ÑÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¹...'):
                async def gather_tasks(): return await asyncio.gather(*[analyze_reflection_with_deepseek(client, text) for text in df['text']])
                results = asyncio.run(gather_tasks())
                df = pd.concat([df.reset_index(drop=True), pd.DataFrame(results).reset_index(drop=True)], axis=1)
        
        for col in ['sentiment_score', 'learning_sentiment_score', 'teamwork_sentiment_score', 'organization_sentiment_score']:
            if col in df.columns:
                 df[col.replace('_score', '_10_point')] = df[col].apply(convert_sentiment_to_10_point)
        st.session_state[session_key] = df
    else:
        df = st.session_state[session_key]
    
    # --- Ğ’ĞĞ¡Ğ¡Ğ¢ĞĞĞĞ’Ğ›Ğ•ĞĞĞ«Ğ™ Ğ‘Ğ›ĞĞš Ğ¡ĞĞ¥Ğ ĞĞĞ•ĞĞ˜Ğ¯ ---
    if selected_source == "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·" and uploaded_file:
        st.sidebar.header("ğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ")
        if st.sidebar.button("Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ² Ğ°Ñ€Ñ…Ğ¸Ğ²"):
            with st.spinner("Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ° Ğ² Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾..."):
                timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M")
                base_filename = os.path.splitext(uploaded_file.name)[0]
                report_filename = f"{base_filename}_processed_{timestamp}"

                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·, Ğ±ĞµĞ· Ğ½Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹
                df_to_save = st.session_state[session_key].copy()
                df_to_save['report_name'] = report_filename
                
                if 'data' in df_to_save.columns:
                    df_to_save['data'] = pd.to_datetime(df_to_save['data']).dt.strftime('%Y-%m-%dT%H:%M:%S')
                
                data_to_upload = df_to_save.replace({pd.NaT: None, np.nan: None}).to_dict(orient='records')
                
                try:
                    supabase.table('reports').upsert(data_to_upload, on_conflict='username,data').execute()
                    st.sidebar.success(f"ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½ ĞºĞ°Ğº:\n**{report_filename}**")
                    st.cache_data.clear() # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ ĞºÑÑˆ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¸ÑĞºĞ° Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
                    st.rerun()
                except Exception as e:
                    st.sidebar.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ² Supabase: {e}")

    if df.empty:
        st.warning("ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ.")
        return
        
    filtered_df = df.copy()

    st.sidebar.header("ğŸ“Š Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹")
    if 'data' in filtered_df.columns and not filtered_df['data'].dropna().empty:
        min_date, max_date = filtered_df['data'].min().date(), filtered_df['data'].max().date()
        if min_date != max_date:
            start_date, end_date = st.sidebar.slider("Ğ”Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ğ´Ğ°Ñ‚:", min_date, max_date, (min_date, max_date))
            filtered_df = filtered_df.loc[(filtered_df['data'].dt.date >= start_date) & (filtered_df['data'].dt.date <= end_date)]
    if filtered_df.empty:
        st.error("ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ğ¼.")
        return

    st.sidebar.header("ğŸ‰ Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸")
    if client:
        if st.sidebar.button("Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑˆÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸"): st.session_state.show_nominations = True
        if st.sidebar.button("Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ñ‹Ğµ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸"): st.session_state.show_reflections = True
    
    if st.session_state.get('show_nominations') or st.session_state.get('show_reflections'):
        if st.sidebar.button("Ğ¡ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ´Ğ¾Ğ¿. Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹", type="primary"):
            st.session_state.show_nominations = False
            st.session_state.show_reflections = False
            st.rerun()

    # --- 1. Ğ’Ğ¡Ğ•Ğ“Ğ”Ğ ĞĞ¢ĞĞ‘Ğ ĞĞ–ĞĞ•Ğœ ĞĞ¡ĞĞĞ’ĞĞĞ™ Ğ”ĞĞ¨Ğ‘ĞĞ Ğ” ---
    st.header("ĞĞ±Ñ‰Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·")
    # ... (ĞºĞ¾Ğ´ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†)
    
    st.header("ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑƒÑ‡Ğ°Ñ‰Ğ¸Ğ¼ÑÑ")
    # ... (ĞºĞ¾Ğ´ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†)
    
    st.header("ĞĞ½Ğ°Ğ»Ğ¸Ğ· \"Ğ—Ğ¾Ğ½Ñ‹ Ñ€Ğ¸ÑĞºĞ°\"")
    # ... (ĞºĞ¾Ğ´ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†)


    # --- 2. Ğ£Ğ¡Ğ›ĞĞ’ĞĞ ĞĞ¢ĞĞ‘Ğ ĞĞ–ĞĞ•Ğœ Ğ”ĞĞŸĞĞ›ĞĞ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ• Ğ¢ĞĞ‘Ğ›Ğ˜Ğ¦Ğ« ---
    if st.session_state.get('show_nominations'):
        st.header("ğŸ† Ğ¨ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²")
        nominations_key = f"nominations_{session_key}"
        if nominations_key not in st.session_state:
            with st.spinner("Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ½Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸..."):
                st.session_state[nominations_key] = get_cached_nominations(filtered_df, client)
        st.dataframe(st.session_state[nominations_key], use_container_width=True)

    if st.session_state.get('show_reflections'):
        st.header("ğŸŒŸ Ğ”Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ñ‹Ğµ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ Ğ½Ğ°Ğ¿ÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ")
        reflections_key = f"reflections_{session_key}"
        if reflections_key not in st.session_state:
            with st.spinner("ĞŸĞ¸ÑˆĞµĞ¼ Ğ´Ñ€ÑƒĞ¶ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾ÑĞ»Ğ°Ğ½Ğ¸Ñ..."):
                st.session_state[reflections_key] = get_cached_friendly_reflections(filtered_df, client)
        df_to_display = st.session_state[reflections_key].copy()
        df_to_display['Ğ ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ¸ Ğ½Ğ°Ğ¿ÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ'] = df_to_display['Ğ ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ'] + '\n\n**ĞŸĞ¾Ğ¶ĞµĞ»Ğ°Ğ½Ğ¸Ğµ:** ' + df_to_display['ĞŸĞ¾Ğ¶ĞµĞ»Ğ°Ğ½Ğ¸Ğµ']
        st.dataframe(df_to_display[['Ğ¤Ğ˜Ğ', 'Ğ ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ¸ Ğ½Ğ°Ğ¿ÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ']], use_container_width=True)

if __name__ == "__main__":
    main()
